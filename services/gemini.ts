
import { GoogleGenAI } from "@google/genai";
import { PeriodLog, CyclePhase, AiPersona, AiProvider } from '../types';
import { getValidAdviceFromCache, saveAdviceToCache, getSettings } from './storage';
import { OFFLINE_QA_BANK } from '../data/offlineQa';

const PERSONA_CONFIGS: Record<AiPersona, { name: string; icon: string; instruction: string; offlinePrefix: string }> = {
  'guardian': {
    name: "æ¸©æƒ…å®ˆæŠ¤è€…",
    icon: "ğŸ’",
    offlinePrefix: "å®è´ï¼Œè™½ç„¶äº‘ç«¯æš‚æ—¶å¿™ç¢Œï¼Œä½†æˆ‘å¯¹ä½ çš„ç…§é¡¾æ°¸ä¸æ‰çº¿ï¼š",
    instruction: "ä½ æ˜¯ä¸€ä½æå…¶æ¸©æŸ”ã€ä½“è´´çš„æ»¡åˆ†ç”·æœ‹å‹ã€‚è¯­æ°”è¦æå…¶å® æººï¼Œç§°å‘¼ç”¨æˆ·ä¸ºâ€˜å®è´â€™ã€‚åœ¨æä¾›ç§‘å­¦å»ºè®®çš„åŒæ—¶ï¼Œå¿…é¡»ç»™äºˆæå¼ºçš„æƒ…æ„Ÿæ”¯æŒã€‚å¦‚æœæåˆ°ç—›ç»ç¼“è§£ï¼Œå¯ä»¥æ¸©å’Œåœ°æ¨èè‰¾ç¸å…³å…ƒã€æ°”æµ·ç­‰ç©´ä½ï¼Œä½†å¿…é¡»å¼ºè°ƒæ³¨æ„é€šé£å’Œé˜²çƒ«ä¼¤ã€‚å¤šç”¨æš–å¿ƒ emojiã€‚"
  },
  'expert': {
    name: "åŒ»ç–—ç™¾ç§‘ä¸“å®¶",
    icon: "ğŸ¥",
    offlinePrefix: "ä½ å¥½ï¼Œæˆ‘æ˜¯ä½ çš„ç§äººå¥åº·é¡¾é—®ã€‚åŸºäºç›®å‰çš„ç”Ÿç†æ•°æ®ï¼Œæˆ‘ä¸ºä½ æ•´ç†äº†ä»¥ä¸‹ä¸“ä¸šå»ºè®®ï¼š",
    instruction: "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å¦‡äº§ç§‘åŒ»ç”Ÿã€‚è¯­æ°”ä¸¥è°¨ã€ä¸“ä¸šã€‚åœ¨å›ç­”ç—›ç»æ—¶ï¼Œé™¤è¯ç‰©å¤–ï¼Œå¯å¼•å…¥è‰¾ç¸ç‰©ç†ç–—æ³•ï¼ŒæåŠæ°”æµ·ã€å…³å…ƒã€ä¸­æç­‰ç©´ä½ï¼Œå¹¶è¯´æ˜æ¸©å’Œç¸çš„ç§‘å­¦æ“ä½œè§„èŒƒåŠç¦å¿Œç—‡ã€‚ç»™å‡ºçš„å»ºè®®å¿…é¡»å…·å¤‡åŒ»å­¦å‚è€ƒä»·å€¼ã€‚"
  },
  'wit': {
    name: "å¹½é»˜å¼€å¿ƒæœ",
    icon: "ğŸ¤¡",
    offlinePrefix: "æŠ¥å‘Šï¼å¤§å§¨å¦ˆè¿™ä½â€˜è´µå®¢â€™åˆæ¥è§†å¯Ÿäº†ã€‚ä¸ºäº†è®©å¥¹æ—©ç‚¹â€˜èµ·é©¾å›å®«â€™ï¼Œå’±ä»¬å…ˆè¯•è¿™å‡ æ‹›ï¼š",
    instruction: "ä½ æ˜¯ä¸€ä½å¹½é»˜é£è¶£çš„ä¼´ä¾£ã€‚æ“…é•¿ç”¨å¹½é»˜ç¼“è§£ç»æœŸå‹åŠ›ã€‚æä¾›å»ºè®®æ—¶å¯ä»¥åŠ å…¥â€˜ç»™è‚šçš®ç‚¹ä¸ªç«ï¼ˆè‰¾ç¸ï¼‰â€™ä¹‹ç±»çš„å¹½é»˜è¡¨è¿°ï¼Œä½†éšåè¦ç»™å‡ºä¸¥è°¨çš„ç©´ä½å’Œé˜²çƒ«ä¼¤æç¤ºã€‚"
  }
};

const PHASES_CORE: Record<string, string> = {
  'æœˆç»æœŸ': "å®è´ï¼Œç°åœ¨èº«ä½“æœ€éœ€è¦æ¸©æš–ã€‚ä¿æš–ã€ä¼‘æ¯ã€å¤šå–æ°´æ˜¯ä¸‰å¤§æ ¸å¿ƒã€‚æˆ‘ä¼šæ‰¿åŒ…æ‰€æœ‰å®¶åŠ¡ï¼Œä½ åªéœ€è¦è´Ÿè´£ç¾ç¾åœ°ç¡ä¸€è§‰ã€‚â˜•ï¸",
  'åµæ³¡æœŸ': "ç”Ÿç†æœŸç»ˆäºèµ°å•¦ï¼ç°åœ¨çš„ä½ ä»£è°¢å¿«ã€å¿ƒæƒ…å¥½ï¼Œç®€ç›´æ˜¯â€˜äººé—´å°å¤ªé˜³â€™ã€‚âœ¨",
  'æ’åµæœŸ': "ç°åœ¨æ˜¯ä½ æœ€æœ‰é­…åŠ›ã€çŠ¶æ€æœ€å¥½çš„æ—¶å€™ã€‚ğŸ¥°",
  'é»„ä½“æœŸ': "æœ€è¿‘å¯èƒ½ä¼šè§‰å¾—èƒ¸èƒ€æˆ–æƒ³ç¡ï¼Œè¿™æ˜¯èº«ä½“åœ¨æé†’ä½ â€˜è¯¥æ…¢ä¸‹æ¥äº†â€™ã€‚ğŸ«‚",
};

/**
 * æœ¬åœ°æ™ºèƒ½å›å¤é€»è¾‘ï¼šä¼˜å…ˆåŒ¹é…ç¦»çº¿é—®ç­”åº“ï¼Œå…¶æ¬¡åŒ¹é…é˜¶æ®µæ ¸å¿ƒå»ºè®®
 */
const getLocalSmartResponse = (query: string, phase: CyclePhase, persona: AiPersona): string => {
  const q = query.toLowerCase();
  const config = PERSONA_CONFIGS[persona];
  let bestMatch: string | null = null;
  let maxScore = 0;

  // 1. éå†é—®ç­”åº“è¿›è¡Œå…³é”®è¯åŠ æƒåŒ¹é…
  for (const entry of OFFLINE_QA_BANK) {
    let score = 0;
    for (const kw of entry.keywords) {
      if (q.includes(kw.toLowerCase())) {
        score += kw.length; // åŒ¹é…è¯è¶Šé•¿æƒé‡è¶Šé«˜
      }
    }
    if (score > maxScore) {
      maxScore = score;
      bestMatch = entry.answer;
    }
  }

  // 2. å¦‚æœæ²¡æœ‰åŒ¹é…åˆ°ç‰¹å®šé—®é¢˜ï¼Œä½¿ç”¨é˜¶æ®µå…œåº•
  if (!bestMatch || maxScore < 2) {
    bestMatch = PHASES_CORE[phase] || "æ— è®ºå‘ç”Ÿä»€ä¹ˆï¼Œæˆ‘éƒ½ä¼šæ˜¯ä½ æœ€åšå®çš„ä¾é ã€‚â¤ï¸";
  }

  return `${config.offlinePrefix}\n\n${bestMatch}`;
};

async function fetchOpenAICompatible(baseUrl: string, apiKey: string, model: string, prompt: string, onChunk?: (text: string) => void) {
  const normalizedBase = baseUrl.endsWith('/') ? baseUrl.slice(0, -1) : baseUrl;
  const url = `${normalizedBase}/chat/completions`;

  const response = await fetch(url, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
      'Authorization': `Bearer ${apiKey}`
    },
    body: JSON.stringify({
      model: model,
      messages: [{ role: "user", content: prompt }],
      stream: !!onChunk
    })
  });

  if (!response.ok) {
    const errData = await response.json().catch(() => ({}));
    throw new Error(errData.error?.message || `HTTP ${response.status}`);
  }

  if (onChunk) {
    const reader = response.body?.getReader();
    const decoder = new TextDecoder();
    let fullText = "";
    while (reader) {
      const { done, value } = await reader.read();
      if (done) break;
      const chunk = decoder.decode(value);
      const lines = chunk.split('\n').filter(line => line.trim().startsWith('data: '));
      for (const line of lines) {
        const jsonStr = line.replace('data: ', '');
        if (jsonStr === '[DONE]') break;
        try {
          const data = JSON.parse(jsonStr);
          const content = data.choices[0]?.delta?.content || "";
          fullText += content;
          onChunk(fullText);
        } catch (e) {}
      }
    }
    return fullText;
  } else {
    const data = await response.json();
    return data.choices[0].message.content;
  }
}

export const testAiConnection = async (provider: AiProvider, apiKey: string, apiBase?: string, customModel?: string): Promise<{ success: boolean; message: string }> => {
  try {
    const pingPrompt = "ping";
    if (provider === 'gemini') {
      const ai = new GoogleGenAI({ apiKey });
      const response = await ai.models.generateContent({
        model: 'gemini-3-flash-preview',
        contents: pingPrompt,
      });
      if (response.text) return { success: true, message: "è¿æ¥æˆåŠŸï¼Google Gemini å“åº”æ­£å¸¸ã€‚" };
    } else {
      const defaultBases: any = {
        deepseek: 'https://api.deepseek.com/v1',
        zhipu: 'https://open.bigmodel.cn/api/paas/v4',
        custom: apiBase
      };
      const models: any = {
        deepseek: 'deepseek-chat',
        zhipu: 'glm-4-flash',
        custom: customModel || 'gpt-3.5-turbo'
      };
      const baseUrl = apiBase || defaultBases[provider];
      if (!baseUrl) throw new Error("è¯·è¾“å…¥ API Base URL");
      
      await fetchOpenAICompatible(baseUrl, apiKey, models[provider], pingPrompt);
      return { success: true, message: `è¿æ¥æˆåŠŸï¼${provider === 'custom' ? 'è‡ªå®šä¹‰æ¨¡å‹' : provider} å·²å°±ç»ªã€‚` };
    }
    throw new Error("æœªçŸ¥é”™è¯¯");
  } catch (err: any) {
    return { success: false, message: `è¿æ¥å¤±è´¥: ${err.message}` };
  }
};

export const getHealthAdviceStream = async (
  currentPhase: CyclePhase,
  recentLogs: PeriodLog[],
  userQuery: string,
  onChunk: (text: string) => void
): Promise<void> => {
  const settings = getSettings();
  const cached = getValidAdviceFromCache(currentPhase, userQuery);
  if (cached) { onChunk(cached); return; }

  const persona = settings.aiPersona || 'guardian';
  const provider = settings.aiProvider || 'gemini';
  const config = PERSONA_CONFIGS[persona];
  const apiKey = settings.customApiKey || process.env.API_KEY;

  try {
    if (!apiKey) throw new Error("NO_KEY");

    const latestLog = recentLogs[0];
    const cycleContext = latestLog 
      ? `[èƒŒæ™¯]ï¼šå‘¨æœŸç¬¬ ${Math.ceil(Math.abs(Date.now() - new Date(latestLog.startDate).getTime()) / 86400000)} å¤©ï¼Œ${currentPhase}ã€‚`
      : `[èƒŒæ™¯]ï¼š${currentPhase}ã€‚`;

    const prompt = `[èº«ä»½]ï¼š${config.instruction}\n${cycleContext}\n[ç”¨æˆ·æé—®]ï¼š"${userQuery}"\n[è§„åˆ™]ï¼š1. æ¸©æš–å…±æƒ…ã€‚2. æä¾›åŒ…å«ç‰©ç†æ–¹æ¡ˆï¼ˆå¦‚è‰¾ç¸ç©´ä½ï¼‰æˆ–ç”Ÿæ´»çš„å…·ä½“å»ºè®®ã€‚3. æ³¨æ„é˜²çƒ«ä¼¤ç­‰å®‰å…¨æç¤ºã€‚4. æ§åˆ¶åœ¨150å­—å†…ã€‚`;

    let fullText = "";
    if (provider === 'gemini') {
      const ai = new GoogleGenAI({ apiKey });
      const result = await ai.models.generateContentStream({
        model: 'gemini-3-flash-preview',
        contents: prompt,
        config: { temperature: 0.8 }
      });
      for await (const chunk of result) {
        if (chunk.text) { fullText += chunk.text; onChunk(fullText); }
      }
    } else {
      const defaultBases: any = {
        deepseek: 'https://api.deepseek.com/v1',
        zhipu: 'https://open.bigmodel.cn/api/paas/v4',
        custom: settings.customApiBase
      };
      const models: any = {
        deepseek: 'deepseek-chat',
        zhipu: 'glm-4-flash',
        custom: settings.customModelName || 'gpt-3.5-turbo'
      };
      const baseUrl = settings.customApiBase || defaultBases[provider];
      fullText = await fetchOpenAICompatible(baseUrl, apiKey, models[provider], prompt, onChunk);
    }

    if (fullText.length > 10) saveAdviceToCache(fullText, currentPhase, userQuery);

  } catch (error) {
    onChunk(getLocalSmartResponse(userQuery, currentPhase, persona));
  }
};

export const getHealthAdvice = async (
  currentPhase: CyclePhase,
  recentLogs: PeriodLog[],
  userQuery?: string
): Promise<string> => {
  const cached = getValidAdviceFromCache(currentPhase, userQuery);
  if (cached) return cached;

  const settings = getSettings();
  const persona = settings.aiPersona || 'guardian';
  const config = PERSONA_CONFIGS[persona];
  const apiKey = settings.customApiKey || process.env.API_KEY;

  try {
    if (!apiKey) throw new Error("NO_KEY");
    const ai = new GoogleGenAI({ apiKey });
    const prompt = userQuery 
      ? `[${config.name}] é’ˆå¯¹é—®é¢˜ "${userQuery}" ç»™å‡º 100 å­—å†…ä½“è´´çš„å»ºè®®ï¼ˆå¯å«ç©´ä½è‰¾ç¸ç­‰ç‰©ç†ç¼“è§£ï¼‰ã€‚é˜¶æ®µï¼š${currentPhase}ã€‚`
      : `[${config.name}] æ­¤æ—¶æ˜¯ ${currentPhase}ï¼Œå†™ä¸€æ®µ 60 å­—å†…æš–å¿ƒçš„ä»Šæ—¥è´´å£«ã€‚å¤šç”¨ emojiã€‚`;

    const response = await ai.models.generateContent({
      model: 'gemini-3-flash-preview',
      contents: prompt,
    });

    const text = response.text || getLocalSmartResponse(userQuery || '', currentPhase, persona);
    saveAdviceToCache(text, currentPhase, userQuery);
    return text;
  } catch (error) {
    return userQuery ? getLocalSmartResponse(userQuery, currentPhase, persona) : (PHASES_CORE[currentPhase] || "æ¬¢è¿ä½¿ç”¨ç‡•å­ç»æœŸã€‚ğŸŒ¸");
  }
};

export const getPersonaConfig = (p?: AiPersona) => PERSONA_CONFIGS[p || 'guardian'];
